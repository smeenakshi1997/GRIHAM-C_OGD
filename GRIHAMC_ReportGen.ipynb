{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smeenakshi1997/GRIHAM-C_OGD/blob/main/GRIHAMC_ReportGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GJXhnktCBS2"
      },
      "source": [
        "### **GRIHAM-C Report Generator code**\n",
        "\n",
        "*** This code is for demonstration purposes and generates the classification_report.json file, one id at a time. This code can be modified slightly and automated completely on the backend of the existing Grievance submission portal.**\n",
        "\n",
        "*** Consists of a separate translation generation code which can be utilised for other language grievance classification. Refer to the attached files and README in github**\n",
        "\n",
        "* All the code in this system is developed and tested for demonstartion purposes only and is developed keeping in mind of extensions further with simple executions that do not require any alterations to the functioning of the clustering system itself.\n",
        "* All the code in this software named GRIHAM-C has been independantly developed, tested and presented by me. Unauthorised use of this software is prohibited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_r56yKZTlQv",
        "outputId": "6054f8be-4433-454c-96f8-d8af3cfe44b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWjbMeecmDxD",
        "outputId": "a4caa8ff-b9f2-472f-8f2f-c4ceae412f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed TensorFlow-2.15.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93skQBgATcHO",
        "outputId": "1c963f46-c2c4-40ee-b65d-25276d1be3e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers  #==4.37.1\n",
        "!pip install torch   #==2.2.0\n",
        "!pip install tqdm\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urzgmJQMUnas",
        "outputId": "446aa8f1-461c-49b4-a33e-79d41fb87882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.37.2\n",
            "Torch version: 2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# Make sure to check the version of transformers and torch while training the model. Re-install if the versions dont match by un-commenting the verions in the previous cell.\n",
        "import transformers\n",
        "import torch\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"Torch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EC8Dws3TrdD",
        "outputId": "c12cdab3-c641-4646-aeaa-14f7ba040ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForSequenceClassification.\n",
            "\n",
            "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import torch\n",
        "from transformers import pipeline, TFBartForSequenceClassification, BartTokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "loaded_model = TFBartForSequenceClassification.from_pretrained('/content/problem_statement_1_and_2/cluster_model_worksfine')\n",
        "loaded_tokenizer = BartTokenizer.from_pretrained('/content/problem_statement_1_and_2/cluster_model_worksfine')\n",
        "\n",
        "map_location=torch.device('cpu')\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=loaded_model, tokenizer=loaded_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW-rolyKsltW"
      },
      "source": [
        "Use the code below for the pickle file of the same model on an GPU machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXXprLRIsg_p"
      },
      "outputs": [],
      "source": [
        "# Can be used with the finetuned model as pickle file\n",
        "# Load the fine-tuned model and tokenizer from the pickle file\n",
        "#with open('/content/drive/MyDrive/problem_statement_1_and_2/cluster_model_worksfine.pkl', 'rb') as file:\n",
        "#     saved_model_data = pickle.load(file)\n",
        "\n",
        "#loaded_model = saved_model_data['model']\n",
        "#loaded_tokenizer = saved_model_data['tokenizer']\n",
        "\n",
        "#loaded_model.to('cpu')\n",
        "\n",
        "#classifier = pipeline(\"zero-shot-classification\", model=loaded_model, tokenizer=loaded_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEL4fyevTwxb"
      },
      "outputs": [],
      "source": [
        "def classification (text, stagelabels):\n",
        "    top5 = []\n",
        "    candidate_labels = stagelabels #catdf['stage2'] = ['Direct Taxes', 'PAN issues', 'Establishment issues']\n",
        "    # Tokenize the input string\n",
        "    tokenizer = loaded_tokenizer\n",
        "\n",
        "    tokenized_input = tokenizer(text, return_tensors=\"pt\")\n",
        "    # Now `tokenized_input` contains the necessary tokens for the model\n",
        "\n",
        "    output = classifier(text, candidate_labels, multi_label=True)\n",
        "\n",
        "    if (len(output['labels'])) > 5:\n",
        "       for m in range (0,4):\n",
        "          if output['labels']:\n",
        "             top5.append(output['labels'][m])\n",
        "    elif (len(output['labels'])) <= 5 and (len(output['labels'])) != 0:\n",
        "         top5 = output['labels']\n",
        "    else:\n",
        "         top5 = []\n",
        "\n",
        "    return (output['labels'][0], top5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7rtk1SmT0ii"
      },
      "outputs": [],
      "source": [
        "def classify_items(json_data, org_code, sequence_to_classify): # instead of org_code stage 3 description can be given\n",
        "    # Convert JSON data to a list of dictionaries\n",
        "    data = json.loads(json_data)\n",
        "\n",
        "    if org_code.startswith(\"GOV\"):\n",
        "       data = data\n",
        "       original_data = data.copy()\n",
        "    else:\n",
        "       # Filter data by OrgCode\n",
        "       data = [item for item in data if item['OrgCode'] == org_code]\n",
        "       original_data = data.copy()\n",
        "       # data = [item for item in data if item['Parent'] == (item ['Code'] if item['Description'] == stage3_description)] i.e. find the code of the stage3 item description\n",
        "\n",
        "    # Initialize result dictionary\n",
        "    result = {}\n",
        "\n",
        "    text = sequence_to_classify\n",
        "\n",
        "    for stage in range(1, 9): # Alter the stage no according to the stage 3 description\n",
        "        # Get items for the current stage\n",
        "        stage_items = [item for item in data if item['Stage'] == stage]\n",
        "        #print(stage_items)\n",
        "\n",
        "        # If there are no items for the current stage, break the loop\n",
        "        if not stage_items:\n",
        "            break\n",
        "\n",
        "        # Get the descriptions of the items\n",
        "        descriptions = [item['Description'] for item in stage_items] #DescriptionHindi\n",
        "\n",
        "        # Classify the descriptions\n",
        "        best_item, top_5_items = classification(text, descriptions)\n",
        "\n",
        "        # Add the best item to the result\n",
        "        result[f'Stage {stage}'] = {\n",
        "            'Best Item': best_item,\n",
        "            'Top 5 Items': top_5_items\n",
        "        }\n",
        "\n",
        "        #print(best_item)\n",
        "        with open('result.json', 'w') as f:\n",
        "             json.dump(result, f)\n",
        "\n",
        "        for item in data:\n",
        "            if item['Description'] == best_item:\n",
        "               best_item_code = item['Code']\n",
        "               break\n",
        "        result[f'Stage {stage}'].update({\"Best Item Code\":best_item_code})\n",
        "        # Update data to only include items whose parent is the code of the best item\n",
        "        data = [item for item in original_data if item['Parent'] == best_item_code]\n",
        "        #print(data)\n",
        "        #print(output)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGJ1_Sp4c36m"
      },
      "source": [
        "After running the following cell, enter the Grievance code and check the language of the text submitted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0zlcgFGUPTF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/problem_statement_1_and_2/no_pii_grievance.json', 'r') as f:\n",
        "     grievance_df = json.load(f)\n",
        "\n",
        "with open('/content/problem_statement_1_and_2/CategoryCode_Mapping_v2.json', 'r') as c:\n",
        "     # Load JSON data from file\n",
        "     json_dat = json.load(c)    # Your JSON data\n",
        "\n",
        "json_data = json.dumps(json_dat)\n",
        "result = []\n",
        "\n",
        "gdf = {\"code\":[], \"text\":[]}\n",
        "\n",
        "gr_id =  input(\"Enter the Grievance Id: \")#\"MEAPD/E/2023/0000002\" #\"GOVUP/E/2023/0000051\" #\"CBODT/E/2023/0000001\" #\"MOLBR/E/2023/0000004\" #\"MORLY/E/2023/0000001\" #\"MOLBR/E/2023/0000003\"\n",
        "for id in grievance_df:\n",
        "    if id['_id'] == gr_id:\n",
        "       gdf[\"code\"] = id['org_code']\n",
        "       gdf[\"text\"] = id['subject_content_text']\n",
        "       break\n",
        "\n",
        "#id = 0 #Id is the only needed to be given usually the unique id of the grievance.the id will fetch all the details from the DB including org code\n",
        "\n",
        "sequence_to_classify = gdf['text'] #Text to classify\n",
        "org_code = gdf['code'] #OrgCode\n",
        "\n",
        "print(sequence_to_classify)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2sb8Dc5bptu"
      },
      "source": [
        "***Check the output to check whether the string is in English or other language. If in other language then translate it to English, from the cell below after choosing the input language.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka-v6mJjUTPJ"
      },
      "outputs": [],
      "source": [
        "result = classify_items(json_data, org_code, sequence_to_classify)\n",
        "\n",
        "print(result)\n",
        "print(len(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XuEAVfMXkfO",
        "outputId": "03718321-76e5-4665-e666-5eaee03d74f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Best Items': {'Stage 1': 'Labour and Employment', 'Stage 2': 'Transfer related issues', 'Stage 3': 'Transfer in/out/Form 13/ Online transfer related issues'}}\n"
          ]
        }
      ],
      "source": [
        "best_items_dict = {'Best Items': {}}\n",
        "\n",
        "for stage, values in result.items():\n",
        "    best_item = values['Best Item']\n",
        "    best_items_dict['Best Items'][f'{stage}'] = best_item\n",
        "\n",
        "\n",
        "print(best_items_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTMJvtEZUWye",
        "outputId": "1828fa1b-69be-4f51-c316-a26696614029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labour and Employment >> Transfer related issues >> Transfer in/out/Form 13/ Online transfer related issues\r\n",
            "\r\n",
            "Name and Address of Establishment : PERSOLKELLY INDIA PRIVATE LIMITED\r\n",
            "UAN No. : X0X3X9X5X8X0\r\n",
            "PF Code/ PF Account No. : X-X-X-X-X\r\n",
            "PPO No. : Not Provided\r\n",
            "Scheme Certificate Number : Not Provided\r\n",
            "PF Office : Regional Office, Bandra 1\r\n",
            "-----------------------\r\n",
            "Hello dear sir madam\r\n",
            "My name is SYED SHEBAAZ AHMED\r\n",
            "My uan number is X0X3X9X5X8X0\r\n",
            "My pf number is X-X-X-X-X\r\n",
            "My company name is PERSOLKELLY INDIA PRIVATE LIMITED\r\n",
            "Hers about company details\r\n",
            "\r\n",
            "Sir i apply for transfer request but epfo still rejected reason is Break statement\r\n",
            "Rejected reason: Your Claim  Claim Id - X-X-X-X-X  has been rejected due to : 1) PAYMENT NOT RECEIVED FOR THE MONTH OF 05/2022 CLARIFY THE SAME \r\n",
            "\r\n",
            "So i contact to my Hr he&#39;s given to me Clearfication Letter break statment and also sending Epfo office regarding\r\n",
            "So i request you to Epfo department please approve my transfer\r\n",
            "Please find the attachment pdf file\r\n",
            "Please reply as soon as possible\n",
            "MOLBR\n"
          ]
        }
      ],
      "source": [
        "print(sequence_to_classify)\n",
        "print(org_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL1fOsK2UzaR"
      },
      "source": [
        "## **Summarizer system for Report Generation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM8L3rENUu4k",
        "outputId": "b763a093-577f-4288-b539-bf75d05657a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: Labour and Employment >> Transfer related issues >> Transfer in/out/Form 13/ Online transfer related issues. Name and Address of Establishment : PERSOLKELLY INDIA PRIVATE LIMITED. Uan No. : X0X3X9X5X8X0.PF Code/ PF Account No.  : X-X-X X-Z.Scheme Certificate Number : Not Provided.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "files = '/content/drive/MyDrive/bart_large_cnn_fine_tuned_summarizer'\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(files)\n",
        "tokenizer = BartTokenizer.from_pretrained(files)\n",
        "\n",
        "# Example text for summarization\n",
        "input_text = sequence_to_classify\n",
        "# Tokenize and generate summary\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=False)\n",
        "summary_ids = model.generate(**inputs)\n",
        "\n",
        "# Decode the summary\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated summary\n",
        "print(\"Generated Summary:\", summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syF-Idjnf4LP"
      },
      "source": [
        "## **Generate Report as JSON file as soon as the grievance is received**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw_MPcGT9u-K",
        "outputId": "999661f5-2d95-447c-abf3-aeac81e51a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON object of classication added successfully and classification_report.json generated!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load existing JSON file\n",
        "existing_file_path = '/content/problem_statement_1_and_2/classification_report.json'\n",
        "try:\n",
        "    with open(existing_file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, start with an empty list\n",
        "    data = []\n",
        "\n",
        "# Create a new JSON object to add\n",
        "classification_report = {\n",
        "    \"grievance_id\": gr_id,\n",
        "    \"org_code\": org_code,\n",
        "    \"grievance_text\": sequence_to_classify,\n",
        "    \"classification_result\": result,\n",
        "    \"best_items_dict\": best_items_dict,\n",
        "    \"summary_result\": summary,\n",
        "    \"remarks_text\": '',\n",
        "    \"translated_text\": ''\n",
        "}\n",
        "# Append the new JSON object to the existing array\n",
        "data.append(classification_report)\n",
        "\n",
        "# Write the updated data back to the JSON file\n",
        "with open(existing_file_path, 'w') as file:\n",
        "    json.dump(data, file, indent=2)\n",
        "\n",
        "print(\"JSON object of classication added successfully and classification_report.json generated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guIXQFukU9pR"
      },
      "source": [
        "## **Additional Independent Translator for Indian Langauges (Except Odia): Use this cell to translate the sequence_to_classify for other language**\n",
        "Use the cells below to continue...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUe7RWI1VJAQ",
        "outputId": "2ff92ee5-83ef-42c0-c0ed-48d5a9818686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ayush >> Miscellaneous >> Others ----------------------- In service, Mr. Pranam, Subject: - In relation to an important information, Mr., I am a resident of Maharaja Dube, Gram - Hasanpurwa, Siwan (Bihar, 841286). I have enrolled in the DYT course “Mr. Padnjali Institute of Yoga and Natural Medicine (MPYPCS)” Chhapra, (Bihar, 841301). The people of this institution say that these institutions are recognised by the Yoga Certification Board (YCB), Indian Yoga Association (IYA), Ministry of Ayush (MoY) and UGC. Sir, is this institution recognised by all the above mentioned institutions and will its degree be recognised across India? Please help me and guide me as I have yet to take a graduate degree in Yoga. Please help. Thank you! Waiting for']\n"
          ]
        }
      ],
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "#article_hi = \"संयुक्त राष्ट्र के प्रमुख का कहना है कि सीरिया में कोई सैन्य समाधान नहीं है\"\n",
        "\n",
        "article = sequence_to_classify\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "lang_list = [\"hi_IN\", \"ta_IN\", \"ml_IN\" , \"mr_IN\", \"bn_IN\", \"te_IN\"]\n",
        "\n",
        "tokenizer.src_lang = \"hi_IN\"\n",
        "encoded_hi = tokenizer(article, return_tensors=\"pt\")\n",
        "generated_tokens = model.generate(\n",
        "    **encoded_hi,\n",
        "    forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
        ")\n",
        "translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(translation)\n",
        "\n",
        "#sequence_to_classify = translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf6F83orVQOZ",
        "outputId": "b16ff72b-8298-491d-9aa7-f26b3e2a96b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Text:  Ayush >> Miscellaneous >> Others ----------------------- In service, Mr. Pranam, Subject: - In relation to an important information, Mr., I am a resident of Maharaja Dube, Gram - Hasanpurwa, Siwan (Bihar, 841286). I have enrolled in the DYT course “Mr. Padnjali Institute of Yoga and Natural Medicine (MPYPCS)” Chhapra, (Bihar, 841301). The people of this institution say that these institutions are recognised by the Yoga Certification Board (YCB), Indian Yoga Association (IYA), Ministry of Ayush (MoY) and UGC. Sir, is this institution recognised by all the above mentioned institutions and will its degree be recognised across India? Please help me and guide me as I have yet to take a graduate degree in Yoga. Please help. Thank you! Waiting for\n"
          ]
        }
      ],
      "source": [
        "#translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(\"Translated Text: \", translation[0])\n",
        "\n",
        "sequence_to_classify_trans = translation[0]\n",
        "\n",
        "#sequence_to_classify = translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46G2b5l_qZPB"
      },
      "source": [
        "# **Classification for translated text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmqWUiuqqRjq",
        "outputId": "6dfa756d-49c7-4026-90f7-5d59502f7ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Stage 1': {'Best Item': 'Ayush', 'Top 5 Items': ['Ayush'], 'Best Item Code': 22801}, 'Stage 2': {'Best Item': 'Miscellaneous', 'Top 5 Items': ['Miscellaneous', 'Ayush Schemes', 'Ayush Education in National Institutes', 'Ayush Research'], 'Best Item Code': 23194}, 'Stage 3': {'Best Item': 'Others', 'Top 5 Items': ['Others', 'All Sections of the MoA', 'All field Organizations of the MoA'], 'Best Item Code': 23197}}\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "result = classify_items(json_data, org_code, sequence_to_classify_trans)\n",
        "\n",
        "print(result)\n",
        "print(len(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vo0sBgv-MLA",
        "outputId": "882af008-68fa-46e8-9ac1-dd0f0432653f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Best Items': {'Stage 1': 'Ayush', 'Stage 2': 'Miscellaneous', 'Stage 3': 'Others'}}\n"
          ]
        }
      ],
      "source": [
        "best_items_dict = {'Best Items': {}}\n",
        "\n",
        "for stage, values in result.items():\n",
        "    best_item = values['Best Item']\n",
        "    best_items_dict['Best Items'][f'{stage}'] = best_item\n",
        "\n",
        "\n",
        "print(best_items_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkX6x27KqlLO"
      },
      "source": [
        "# **Summary for translated Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IIUCDOjqYHY",
        "outputId": "25bb3b69-cc18-4e16-daec-ddee2790bf30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: Mr. Pranam is a resident of Maharaja Dube, Gram - Hasanpurwa, Siwan (Bihar, 841286) He has enrolled in the DYT course “Mr. Padnjali Institute of Yoga and Natural Medicine (MPYPCS)” Chhapra, (B Bihar, 841301) The people of this institution say that these institutions are recognised by the Yoga Certification Board.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "files = '/content/drive/MyDrive/bart_large_cnn_fine_tuned_summarizer'\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(files)\n",
        "tokenizer = BartTokenizer.from_pretrained(files)\n",
        "\n",
        "# Example text for summarization\n",
        "input_text = sequence_to_classify_trans\n",
        "# Tokenize and generate summary\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=False)\n",
        "summary_ids = model.generate(**inputs)\n",
        "\n",
        "# Decode the summary\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated summary\n",
        "print(\"Generated Summary:\", summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LetptWnWrBKR"
      },
      "source": [
        "# **Report for Translated text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxbxQZMJrAR7",
        "outputId": "31003093-855c-48ed-b57c-81af51a54918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON object of classication added successfully and classification_report.json generated!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load existing JSON file\n",
        "existing_file_path = '/content/classification_report_hindi.json' #'/content/classification_report_hindi.json'\n",
        "try:\n",
        "    with open(existing_file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, start with an empty list\n",
        "    data = []\n",
        "\n",
        "# Create a new JSON object to add\n",
        "classification_report = {\n",
        "    \"grievance_id\": gr_id,\n",
        "    \"org_code\": org_code,\n",
        "    \"grievance_text\": sequence_to_classify,\n",
        "    \"classification_result\": result,\n",
        "    \"best_items_dict\": best_items_dict,\n",
        "    \"summary_result\": summary,\n",
        "    \"remarks_text\": '',\n",
        "    \"translated_text\": sequence_to_classify_trans\n",
        "}\n",
        "# Append the new JSON object to the existing array\n",
        "data.append(classification_report)\n",
        "\n",
        "# Write the updated data back to the JSON file\n",
        "with open(existing_file_path, 'w') as file:\n",
        "    json.dump(data, file, indent=2)\n",
        "\n",
        "print(\"JSON object of classication added successfully and classification_report.json generated!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf9PYnvzT0oi"
      },
      "outputs": [],
      "source": [
        "'''#To fetch the classifying items from the json file with Description in Hindi. Created for future instance.\n",
        "def classify_items(json_data, org_code, sequence_to_classify): # Hindi: instead of org_code stage 3 description can be given\n",
        "    # Convert JSON data to a list of dictionaries\n",
        "    data = json.loads(json_data)\n",
        "\n",
        "    if org_code.startswith(\"GOV\"):\n",
        "       data = data\n",
        "       original_data = data.copy()\n",
        "    else:\n",
        "       # Filter data by OrgCode\n",
        "       data = [item for item in data if item['OrgCode'] == org_code]\n",
        "       original_data = data.copy()\n",
        "       # data = [item for item in data if item['Parent'] == (item ['Code'] if item['Description'] == stage3_description)] i.e. find the code of the stage3 item description\n",
        "\n",
        "    #print(data)\n",
        "\n",
        "    # Initialize result dictionary\n",
        "    result = {}\n",
        "\n",
        "    text = sequence_to_classify\n",
        "\n",
        "    for stage in range(1, 9): # Alter the stage no according to the stage 3 description\n",
        "        # Get items for the current stage\n",
        "        stage_items = [item for item in data if item['Stage'] == stage]\n",
        "        #print(stage_items)\n",
        "\n",
        "        # If there are no items for the current stage, break the loop\n",
        "        if not stage_items:\n",
        "            break\n",
        "\n",
        "        # Get the descriptions of the items\n",
        "        descriptions = [item['DescriptionHindi'] for item in stage_items] #DescriptionHindi\n",
        "\n",
        "        # Classify the descriptions\n",
        "        output, best_item, top_5_items = classification(text, descriptions)\n",
        "\n",
        "        # Add the best item to the result\n",
        "        result[f'Stage {stage}'] = {\n",
        "            'Best Item': best_item,\n",
        "            'Top 5 Items': top_5_items\n",
        "        }\n",
        "        print(best_item)\n",
        "        with open('result.json', 'w') as f:\n",
        "             json.dump(result, f)\n",
        "\n",
        "        for item in data:\n",
        "            if item['DescriptionHindi'] == best_item:\n",
        "               best_item_code = item['Code']\n",
        "               break\n",
        "        print(best_item_code)\n",
        "        # Update data to only include items whose parent is the code of the best item\n",
        "        data = [item for item in original_data if item['Parent'] == best_item_code]\n",
        "        print(data)\n",
        "        print(output)\n",
        "\n",
        "    return result'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lawpzt0UUXdB"
      },
      "outputs": [],
      "source": [
        "'''import json\n",
        "\n",
        "def add_description_hindi(json_data):\n",
        "  \"\"\"\n",
        "  Adds a \"DescriptionHindi\" key with an empty string value to JSON objects that do not have it.\n",
        "\n",
        "  Args:\n",
        "    json_data: A list of JSON objects.\n",
        "\n",
        "  Returns:\n",
        "    A list of JSON objects with the \"DescriptionHindi\" key added.\n",
        "  \"\"\"\n",
        "\n",
        "  updated_data = []\n",
        "  for obj in json_data:\n",
        "    if \"Destination\" not in obj:\n",
        "      obj[\"Destination\"] = \"\"\n",
        "    updated_data.append(obj)\n",
        "\n",
        "  return updated_data\n",
        "\n",
        "# Load the JSON data from a file\n",
        "with open('/content/drive/MyDrive/problem_statement_1_and_2/CategoryCode_Mapping_v2.json', 'r') as f:\n",
        "  json_data = json.load(f)\n",
        "\n",
        "# Add the \"DescriptionHindi\" key\n",
        "updated_data = add_description_hindi(json_data)\n",
        "\n",
        "# Save the updated JSON data to a file\n",
        "with open('/content/drive/MyDrive/problem_statement_1_and_2/CategoryCode_Mapping_v2.json', 'w') as f:\n",
        "  json.dump(updated_data, f)\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}